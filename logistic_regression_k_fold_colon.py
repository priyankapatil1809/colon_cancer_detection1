# -*- coding: utf-8 -*-
"""Logistic_Regression_k-fold_colon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuc4lF061KxYqL3xsv5O7SBhCOe_zRjm
"""

from google.colab import drive

drive.mount('/content/drive',timeout_ms=60000)

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_recall_curve, average_precision_score
import seaborn as sns
import matplotlib.pyplot as plt
from skimage.feature import hog, local_binary_pattern

# Define your dataset directory
dataset_dir = '/content/drive/MyDrive/Colon_use'

# Initialize empty lists for images and labels
images = []
labels = []

# Define a mapping of class labels to integers
class_to_int = {'cancer_y': 0, 'cancer_n': 1}

# Loop through your dataset directory to load images and labels
for category in os.listdir(dataset_dir):
    category_dir = os.path.join(dataset_dir, category)
    for img_filename in os.listdir(category_dir):
        img_path = os.path.join(category_dir, img_filename)
        img = cv2.imread(img_path)
        img = cv2.resize(img, (256, 256))  # Resize images to a common size
        images.append(img)  # Keep color images
        labels.append(class_to_int[category])  # Map class labels to integers

# Convert lists to NumPy arrays
images = np.array(images)
labels = np.array(labels)

# Define function to extract HOG features from an image
def extract_hog_features(image):
    # Convert color image to grayscale
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    features = hog(image_gray, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=False)
    return features

# Define function to extract LBP features from an image
def extract_lbp_features(image):
    radius = 3
    n_points = 24
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    lbp_image = local_binary_pattern(image_gray, n_points, radius, method='uniform')
    lbp_histogram, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))
    lbp_histogram = lbp_histogram / (lbp_histogram.sum() + 1e-6)  # Normalize
    return lbp_histogram

# Apply the feature extraction functions to each image in the dataset
hog_features_train = np.array([extract_hog_features(img) for img in images])
lbp_features_train = np.array([extract_lbp_features(img) for img in images])
rgb_features_train = images.reshape(images.shape[0], -1)  # Flatten RGB images

# Concatenate features from different methods
X_train = np.hstack((hog_features_train, lbp_features_train, rgb_features_train))
y_train = labels

# Initialize KFold cross-validator
num_folds = 5
kf = KFold(n_splits=num_folds, random_state=42, shuffle=True)

# Initialize lists to store evaluation metrics for each fold
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

# Perform k-fold cross-validation
for train_index, test_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]

    # Apply PCA with 2 components to the training and validation data
    pca = PCA(n_components=2)
    X_train_pca_fold = pca.fit_transform(X_train_fold)
    X_val_pca_fold = pca.transform(X_val_fold)

    # Create a Logistic Regression model
    model_pca = LogisticRegression(random_state=42, max_iter=1000)

    # Fit the model to the PCA transformed training data
    model_pca.fit(X_train_pca_fold, y_train_fold)

    # Make predictions on the validation set using PCA transformed features
    y_pred_pca_fold = model_pca.predict(X_val_pca_fold)

    # Evaluate the model with PCA for this fold
    accuracy_fold = accuracy_score(y_val_fold, y_pred_pca_fold)
    accuracy_scores.append(accuracy_fold)

    precision_fold = precision_score(y_val_fold, y_pred_pca_fold)
    precision_scores.append(precision_fold)

    recall_fold = recall_score(y_val_fold, y_pred_pca_fold)
    recall_scores.append(recall_fold)

    f1_fold = f1_score(y_val_fold, y_pred_pca_fold)
    f1_scores.append(f1_fold)

# Calculate average evaluation metrics across all folds
avg_accuracy = np.mean(accuracy_scores)
avg_precision = np.mean(precision_scores)
avg_recall = np.mean(recall_scores)
avg_f1 = np.mean(f1_scores)

# Print average evaluation metrics
print("Average Accuracy with K-fold Cross-Validation:", avg_accuracy)
print("Average Precision with K-fold Cross-Validation:", avg_precision)
print("Average Recall with K-fold Cross-Validation:", avg_recall)
print("Average F1 Score with K-fold Cross-Validation:", avg_f1)